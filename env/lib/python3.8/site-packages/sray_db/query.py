from __future__ import annotations

import typing
from copy import copy, deepcopy
from functools import reduce
from typing import List, Union, Any, Dict

import pandas as pd
from sray_db.apps.app import Field
from sray_db.apps.pk import PrimaryKey

if typing.TYPE_CHECKING:
    import sray_db.broker


class Query:
    """Base class for database queries"""
    def __init__(self):
        raise Exception("Query cannot be instantiated, use one of its implementations (Get, Put) instead")

    def copy(self) -> Query:
        return copy(self)

    def deepcopy(self) -> Query:
        return deepcopy(self)


class Get(Query):
    """Implementation of Query to retrieve data from a DataBroker

    Data is returned in the form of a pd.DataFrame, with the PrimaryKey fields as index columns and all requested
    Fields as columns. When no data is found, a DataFrame with 0 rows is returned

    Arguments
        fields: List of Fields to be queried
        load_pks: when True (default), the PrimaryKeys associated with the requested fields will be returned as the
            DataFrame's index columns. When False, the index will be an automatically generated list of integers
        load_idx: returned results are limited to the indices provided in load_idx, via an inner join
        hierarchy: hierarchy (e.g. 'security' on which the results need to be based
        filter: nested dictionary that contains requirements any returned results need to comply with. Supports gates
            ('and', 'or', as well as various operators such as '==', '!=', 'in', ...). Generates an additional pd.Index
            and adds it to the query's load_idx.
        distinct: if True, only distinct results (based on all queried fields) will be returned
        as_of: if provided, limits the result set to datapoints that have been published on or before the as_of date

    Example

        This example constructs a query for the ESG score for all scores that:
            - have an assetid between 1000 - 1999 and are calculated for date '2018-04-24' (-> load_idx) AND
            - ( - (belong to Belgian companies AND are greater than 50) OR
                - have scores between 30 and 35) (-> filter)

        import pandas as pd
        from sray_db.apps import apps
        from sray_db.apps.pk import PrimaryKey

        assets = pd.Index(list(range(1000, 2000)), name=PrimaryKey.assetid)
        dates = pd.DatetimeIndex(['2018-04-24'], name=PrimaryKey.date)
        esg_score = apps['ESG'][(2,6,0,0)]['esg']
        country_name = apps['assetinfo_dom_country'][(1, 0, 0, 0)]['name']

        filter = {
            'or': [
                {'and': [
                    {'==': {country_name: 'Belgium'}},
                    {'>=': {esg_score: 50}}]},
                {'between': {esg_score: (30, 35)}}]}

        query = Get(fields=[esg_score], load_idx=[assets, dates], filter=filter)

    """

    def __init__(self, fields: List[Field] = None, load_pks: bool = True, load_idx: List[pd.Index] = None,
                 hierarchy: str = 'entity', filter: Dict[str, Any] = None, distinct: bool = False,
                 as_of: Union[pd.Timestamp, None] = None):
        self.fields = fields or []
        self.load_pks = load_pks
        self.load_idx = load_idx or []
        self.hierarchy = hierarchy
        self.filter = filter or {}
        self.distinct = distinct
        self.as_of = pd.Timestamp(as_of) if as_of is not None else None

    @property
    def fields(self) -> List[Field]:
        return self._fields

    @fields.setter
    def fields(self, new_fields: List[Field]) -> None:
        assert isinstance(new_fields, (list, tuple)), f"fields must be a list or tuple, is {type(new_fields)}"

        self._fields = new_fields

    @property
    def load_pks(self) -> bool:
        return self._load_pks

    @load_pks.setter
    def load_pks(self, new_load_pks: bool) -> None:
        assert isinstance(new_load_pks, bool), f"load_pks must be a boolean, is {type(new_load_pks)}"

        self._load_pks = new_load_pks

    @property
    def load_idx(self) -> List[pd.MultiIndex]:
        return self._load_idx

    @load_idx.setter
    def load_idx(self, new_load_idx: List[pd.MultiIndex]) -> None:
        self.assert_valid_load_idx(new_load_idx)

        self._load_idx = new_load_idx

    @staticmethod
    def assert_valid_load_idx(load_idx: List[pd.MultiIndex]) -> None:
        assert isinstance(load_idx, (list, tuple)), f"fields must be a list or tuple, is {type(load_idx)}"
        for idx in load_idx:
            assert isinstance(idx, pd.Index), (
                f"All elements in load_idx must be pd.Index or pd.MultiIndex, found {type(idx)}")
            for name in idx.names:
                assert isinstance(name, (Field, PrimaryKey)), (
                    f"All index.names must be be of type Field or Primary, found {type(name)}")

    @property
    def hierarchy(self) -> str:
        return self._hierarchy

    @hierarchy.setter
    def hierarchy(self, new_hierarchy: str) -> None:
        self._hierarchy = new_hierarchy

    @property
    def filter(self) -> Dict[str: Any]:
        return self._filter

    @filter.setter
    def filter(self, new_filter: Dict[str: Any]) -> None:
        assert isinstance(new_filter, dict), f"filter must be of type dict, found {type(new_filter)}"
        assert len(new_filter) <= 1, f"filter can contain no more than one key on the top level, found {len(new_filter)}"

        self._filter = new_filter

    @property
    def distinct(self) -> bool:
        return self._distinct

    @distinct.setter
    def distinct(self, new_distinct: bool) -> None:
        assert isinstance(new_distinct, bool), f"distinct must be of type boolean, found {type(new_distinct)}"
        self._distinct = new_distinct

    def flatten_filter(self, db: sray_db.broker.DataBroker) -> Get:
        """Returns a copy of the Query in which the filter attribute has been flattened to load_idx by querying and
        asserting all requirements"""

        flattened = self.deepcopy()
        filter_idx = flattened._index_from_filter(db)

        all_idx = []
        if filter_idx is not None:
            all_idx += [filter_idx]
        all_idx += flattened.load_idx

        new_idx = []
        if len(all_idx) > 0:
            # Group indices by identical sets of PrimaryKeys
            unique_pk_combs = set([idx.names for idx in all_idx])
            for pks in unique_pk_combs:
                indices = [idx for idx in all_idx if set(idx.names) == set(pks)]
                new_idx.append(reduce((lambda x, y: x.intersection(y)), indices))

        flattened.filter = {}
        flattened.load_idx = new_idx
            
        return flattened

    def _index_from_filter(self, db: sray_db.broker.DataBroker) -> Union[None, pd.MultiIndex]:
        if len(self.filter.keys()) < 1:
            index = None
        elif len(self.filter.keys()) == 1:
            key = next(iter(self.filter))
            arg = self.filter[key]

            index = self.eval_filter_key(db, key, arg)
            has_results = len(index) > 0

            if not has_results:
                raise IndexError('No load index remains after filtering')
        else:
            raise ValueError("Filter needs to be a dictionary with only 1 key at the top level")

        return index

    def eval_filter_key(self, db: sray_db.broker.DataBroker, key: str, value: Any) -> pd.MultiIndex:
        if key in gates.keys():
            result = self.eval_gate(db, key, value)
        elif key in ops.keys():
            result = self.eval_operator(db, key, value)
        else:
            raise KeyError(f'{key} not recognized as filter key')

        return result

    def eval_gate(self, db: sray_db.broker.DataBroker, gate: str, arg: List[Dict[Any]]) -> pd.MultiIndex:
        indices = []
        for dict_ in arg:
            if len(dict_) != 1:
                raise ValueError('Invalid construction of gate dictionary, only 1 key allowed per argument dictionary')
            key = next(iter(dict_))
            val = dict_[key]
            indices.append(self.eval_filter_key(db, key, val))

        multiple_pks = len(set(frozenset(idx.names) for idx in indices)) > 1

        if gate == 'or' and multiple_pks:
            raise ValueError('"or" gate only works with fields with similar primary keys')

        result = reduce(gates[gate], indices)

        return result

    def eval_operator(self, db: sray_db.broker.DataBroker, op: str, arg: Any) -> pd.MultiIndex:
        if len(arg) != 1:
            raise ValueError('Invalid construction of operator dictionary')

        field = next(iter(arg))
        val = arg[field]
        func = ops[op]

        q = self.deepcopy()
        q.fields = [field]
        q.distinct = False
        q.filter = {}

        data = db.query(q)

        f_data = data[func(data[field], val)]

        return f_data.index

    def unique_date_index(self) -> Union[pd.DatetimeIndex, None]:
        """Returns a DatetimeIndex of all unique requested dates in
        self.load_idx, or None if no dates are specified"""

        result = pd.DatetimeIndex([], name=PrimaryKey.date)
        if len(self.load_idx) > 0:
            pk_date = PrimaryKey.date
            date_indices = (idx for idx in self.load_idx if pk_date in idx.names)
            for idx in date_indices:
                result = result.append(idx.get_level_values(pk_date))

        result = result.unique()
        if len(result) == 0:
            result = None

        return result

    def _validate_fields_unique(self) -> None:
        assert len(self.fields) == len(set(self.fields))

gates = {
    'and': (lambda x, y: x.join(y, how='inner')),
    'or': (lambda x, y: x.join(y, how='outer'))}

ops = {
    '==': (lambda x, y: x == y),
    '!=': (lambda x, y: x != y),
    '>': (lambda x, y: x > y),
    '>=': (lambda x, y: x >= y),
    '<': (lambda x, y: x < y),
    '<=': (lambda x, y: x < y),
    'in': (lambda x, y: x in y),
    'notin': (lambda x, y: x not in y),
    'between': (lambda x, y: x.between(y[0], y[1]))
    }


class Put(Query):
    """Implementation of Query to store data into an Interface.

    Arguments
        data: pd.DataFrame with the data to be stored. Index names must be of type PrimaryKey, and column names must be
            of type Field.

    """
    def __init__(self, data: pd.DataFrame, load_cutoff: Union[pd.Timestamp, None] = None,
                 load_idx_to_overwrite: List[pd.Index] = None, chunksize: int = 10000):
        self.data = self.verify_data(data)
        self.load_cutoff = pd.Timestamp(load_cutoff) if load_cutoff is not None else None
        self.load_idx_to_overwrite = load_idx_to_overwrite
        self.chunksize = chunksize

    def verify_data(self, data: pd.DataFrame) -> pd.DataFrame:
        data = self._verify_index_names(data)
        data = self._verify_column_names(data)

        return data

    @staticmethod
    def _verify_index_names(data: pd.DataFrame) -> pd.DataFrame:
        """All index names must be of type PrimaryKey"""
        bad_index = []
        for name in data.index.names:
            if not isinstance(name, PrimaryKey):
                bad_index.append(name)

        if len(bad_index) > 0:
            raise ValueError(f'All index names must be of type PrimaryKey, found {bad_index}')

        return data

    @staticmethod
    def _verify_column_names(data: pd.DataFrame) -> pd.DataFrame:
        """All column names must be of type Field"""
        bad_columns = []
        for name in data.columns:
            if not isinstance(name, Field):
                bad_columns.append(name)

        if len(bad_columns) > 0:
            raise ValueError(f'All column names must be of type PrimaryKey, found {bad_columns}')

        return data

    @property
    def load_cutoff(self) -> Union[pd.Timestamp, None]:
        return self._load_cutoff

    @load_cutoff.setter
    def load_cutoff(self, new_load_cutoff: Union[None, List[pd.MultiIndex]]) -> None:
        load_cutoff_columns = [c for c in self.data.columns if c.name == 'load_cutoff']
        if new_load_cutoff and len(load_cutoff_columns) > 0:
            raise ValueError(
                f'`load_cutoff` is defined as a data column and a query argument, can only be one of both'
            )
        self._load_cutoff = new_load_cutoff

    @property
    def load_idx_to_overwrite(self) -> Union[None, List[pd.MultiIndex]]:
        return self._load_idx_to_overwrite

    @load_idx_to_overwrite.setter
    def load_idx_to_overwrite(self, new_load_idx_to_overwrite: Union[None, List[pd.MultiIndex]]) -> None:
        idxes = new_load_idx_to_overwrite
        if idxes is None:
            self._load_idx_to_overwrite = None
        else:
            Get.assert_valid_load_idx(new_load_idx_to_overwrite)
            self._load_idx_to_overwrite = idxes

        self._load_idx_to_overwrite = idxes


class Delete(Query):
    """Implementation of Query to delete data from an Interface.

    Arguments
        fields: List of Fields to be queried
        load_idx: Deleted results are limited to the indices provided in load_idx, via an inner join


    """
    def __init__(self, fields: List[Field], load_idx: List[pd.Index]):
        self.fields = fields
        self.load_idx = load_idx
