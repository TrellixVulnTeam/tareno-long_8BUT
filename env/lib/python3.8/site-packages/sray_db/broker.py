import logging
from functools import reduce
from queue import Queue
from threading import Thread, Semaphore
from typing import Dict, List, Union

import pandas as pd

from sray_db import config
from sray_db.apps import apps
from sray_db.apps.field import Field
from sray_db.apps.pk import PrimaryKey
from sray_db.interface import DataInterface
from sray_db.psql.cnxn import lock
from sray_db.psql.interface import PSQLTable
from sray_db.query import Query, Get, Put, Delete

log = logging.getLogger(__name__)

entity_app = apps['assetinfo_entity'][(1, 0, 0, 0)]
primary_field = entity_app['primary_assetid']

sema = Semaphore(value=config['SRAYDB_SQL_BROKER_MAX_THREADS'])
que = Queue()


class DataBroker:
    def __init__(self, autoload_interfaces: bool = True):
        self.interfaces = []

        self._ec_map = None
        self._pm_map = None

        if autoload_interfaces is True:
            self._autoload_interfaces()

    def query(self, query: Query, threaded: bool = True) -> pd.DataFrame:
        if isinstance(query, Get):
            try:
                query = query.flatten_filter(self)
            except IndexError:
                return pd.DataFrame(columns=query.fields)

            return self.get(fields = query.fields,
                            distinct = query.distinct,
                            load_idx = query.load_idx,
                            load_pks = query.load_pks,
                            hierarchy = query.hierarchy,
                            as_of = query.as_of,
                            threaded = threaded)
        elif isinstance(query, Put):
            updated = self.put(data=query.data,
                               load_cutoff=query.load_cutoff,
                               load_idx_to_overwrite=query.load_idx_to_overwrite,
                               chunksize=query.chunksize)
            return updated

        elif isinstance(query, Delete):
            deleted = self.delete(query.fields, query.load_idx)
            return deleted

        else:
            raise NotImplementedError

    def get(self, fields: List[Field], distinct: bool, load_idx: List[pd.Index],
            load_pks: bool, hierarchy: str, as_of: Union[pd.Timestamp, None], threaded: bool) -> pd.DataFrame:
        log.debug(f'Getting fields {fields}')

        field_map = self.split_fields_by_interface(fields)

        nr_interfaces = len(field_map.keys())
        if load_pks is False and nr_interfaces > 1:
            raise ValueError('PrimaryKeys not loaded and more than 1 DataInterface is required')

        results = []

        if threaded is True:
            threads = []
            for interface, interface_fields in field_map.items():
                thread = Thread(target=self._get_interface_threaded, kwargs={
                                                                'interface': interface,
                                                                'hierarchy': hierarchy,
                                                                'fields': interface_fields,
                                                                'distinct': distinct,
                                                                'load_idx': load_idx,
                                                                'load_pks': load_pks,
                                                                'as_of': as_of})
                thread.start()
                threads.append(thread)

            for thread in threads:
                thread.join()

            while not que.empty():
                results.append(que.get())

        else:
            for interface, interface_fields in field_map.items():
                result = self._get_interface(
                    interface = interface,
                    hierarchy = hierarchy,
                    fields = interface_fields,
                    distinct = distinct,
                    load_idx = load_idx,
                    load_pks = load_pks,
                    as_of = as_of)
                results.append(result)

        # When outer joining DataFrames with different index.nlevels, a left join is performed instead. To minimize
        # this, we first sort the result DataFrames by descending nlevels so the left join is at least performed on
        # the biggest resultset possible.

        results = sorted(results, key=lambda x: x.index.nlevels, reverse=True)
        data = reduce((lambda x, y: x.join(y, how='outer')), results)

        return data[fields]

    def put(self, data: pd.DataFrame, load_cutoff: Union[pd.Timestamp, None] = None,
            load_idx_to_overwrite: List[pd.Index] = None, chunksize: int = 10000) -> int:

        field_map = self.split_fields_by_interface(data.columns)
        if len(field_map.keys()) > 1:
            raise ValueError(f'Put requests can only target one DataInterface. Instead, got {field_map}')

        interface = next(iter(field_map))
        stored = interface.put(data, load_cutoff, load_idx_to_overwrite, chunksize)

        return stored

    def delete(self, fields: List[Field], load_idx: List[pd.Index]) -> int:

        field_map = self.split_fields_by_interface(fields)
        if len(field_map.keys()) > 1:
            raise ValueError(f'Delete requests can only target one DataInterface. Instead, got {field_map}')

        interface = next(iter(field_map))
        deleted = interface.delete(fields, load_idx)

        return deleted

    def _get_interface(self, interface: DataInterface, hierarchy: str, **kwargs) -> pd.DataFrame:
        load_idx = kwargs.pop('load_idx', [])

        interface_load_index = load_idx.copy()
        if hierarchy != 'security':
            if interface.hierarchy == 'entity':
                interface_load_index = self._indexes_to_primary(interface_load_index)
        result = interface.get(load_idx=interface_load_index, **kwargs)

        if hierarchy != 'security':
            if interface.hierarchy == 'entity':
                asset_indices = [idx for idx in load_idx if PrimaryKey.assetid in idx.names]
                if asset_indices:
                    result = self._expand_to_entcrossed(result, load_idx)

        return result

    def _get_interface_threaded(self, interface: DataInterface, **kwargs) -> None:
        sema.acquire()

        result = self._get_interface(interface, **kwargs)

        que.put(result)
        sema.release()

    def add_interface(self, interface: DataInterface) -> None:
        self.interfaces.append(interface)

    def split_fields_by_interface(self, fields: List[Field]) -> Dict[DataInterface, List[Field]]:
        """Returns a dictionary with for each Interface a list of the requested Fields it can provide"""

        split_fields = {}
        allocated_fields = []
        for interface in self.interfaces:
            reg_fields = [f for f in fields if f in interface.registered_fields]
            if len(reg_fields) > 0:
                split_fields[interface] = reg_fields

                allocated_fields += reg_fields

        if len(allocated_fields) != len(fields):
            unallocated = set(fields).difference(set(allocated_fields))
            unallocated_str = f'{", ".join(str(f) for f in unallocated)}'

            raise ValueError(f'Couldn\'t match {len(unallocated)} fields with a DataInterface: {unallocated_str}')

        return split_fields

    @property
    def ec_map(self) -> pd.Series:
        if self._ec_map is None:
            self._ec_map = self._create_ec_map()
        return self._ec_map

    @property
    def pm_map(self) -> pd.Series:
        if self._pm_map is None:
            self._pm_map = self._create_primary_map()
        return self._pm_map

    def _create_ec_map(self) -> pd.Series:
        pm = self.pm_map.reset_index()
        pm2 = pm.rename(columns={PrimaryKey.assetid: 'EntityAssetID'})

        ec_map = pm.merge(pm2, on=primary_field).drop(primary_field, axis=1)
        ec_map = ec_map.set_index([PrimaryKey.assetid])['EntityAssetID']

        return ec_map

    def _expand_to_entcrossed(self, df: pd.DataFrame, load_idx: List[pd.Index]) -> pd.DataFrame:
        idx_names = df.index.names
        if PrimaryKey.assetid in idx_names:
            df = df.reset_index()
            ec_map = self.ec_map.reset_index()

            crossed = df.merge(ec_map, on=PrimaryKey.assetid, how='left')
            crossed = crossed.drop(PrimaryKey.assetid, axis=1)
            crossed = crossed.rename(columns={'EntityAssetID': PrimaryKey.assetid})

            asset_indices = [idx for idx in load_idx if PrimaryKey.assetid in idx.names]
            for idx in asset_indices:
                matching_index_names = list(set(idx.names).intersection(set(idx_names)))
                idx_df = pd.DataFrame(index=idx).reset_index()[matching_index_names]
                crossed = crossed.merge(idx_df, on=matching_index_names, how='inner')

            crossed = crossed.set_index(idx_names)

        else:
            crossed = df

        return crossed

    def _create_primary_map(self) -> pd.Series:

        q = Get(fields=[primary_field], hierarchy='security')
        pm_map = self.query(q, threaded=False)[primary_field]

        return pm_map

    def _indexes_to_primary(self, indexes: List[pd.MultiIndex]) -> List[pd.MultiIndex]:
        result = []
        for idx in indexes:
            result.append(self._index_to_primary(idx))

        return result

    def _index_to_primary(self, idx: pd.MultiIndex) -> pd.MultiIndex:
        idx_names = idx.names
        if PrimaryKey.assetid in idx.names:
            df = pd.DataFrame(index=idx.copy())
            with lock:
                mapped = df.join(self.pm_map, how='inner')
            mapped = mapped.reset_index(PrimaryKey.assetid, drop=True)
            mapped = mapped.rename(columns={primary_field: PrimaryKey.assetid})
            mapped = mapped.reset_index().set_index(idx_names)

            p_idx = mapped.index.drop_duplicates()

        else:
            p_idx = idx

        return p_idx

    def _autoload_interfaces(self) -> None:
        from sray_db.apps import apps

        for app_name in apps:
            unv_app = apps[app_name]
            for version in unv_app:
                app = unv_app[version]

                interface = PSQLTable.from_app(app)
                self.add_interface(interface)
