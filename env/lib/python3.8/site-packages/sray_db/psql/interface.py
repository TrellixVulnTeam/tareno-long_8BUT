from __future__ import annotations

from typing import List, Union, Dict

import pandas as pd
import sqlalchemy
import sqlalchemy.sql.sqltypes as sqltypes
from sqlalchemy import func, Table, Column, MetaData, Float, Numeric, Integer, Date, DateTime, Time, BigInteger, \
    SmallInteger, tuple_, select, desc
from sqlalchemy.engine.base import Engine

from sray_db import config
from sray_db.interface import DataInterface
from sray_db.psql.cnxn import get_engine, get_meta
from sray_db.psql.table import WideTable
from sray_db.apps.field import Field
from sray_db.apps.registry import App
from sray_db.apps.pk import PrimaryKey


class PSQLTable(DataInterface):

    def __init__(self, engine: Engine, field_column_map: List[Field],
                 hierarchy: str, table: WideTable):
        self.engine = engine
        self.field_column_map = field_column_map
        self.hierarchy = hierarchy
        self.table = table

    @property
    def registered_fields(self) -> List['Field']:
        """Returns a list of Fields that are stored in the DataInterface"""
        return list([f for f in self.field_column_map if not isinstance(f, PrimaryKey)])

    @property
    def primary_keys(self) -> List[PrimaryKey]:
        return [f for f in self.field_column_map.keys() if isinstance(f, PrimaryKey)]

    def get(self, fields: List[Field], distinct: bool, load_idx: List[pd.Index] = None,
            load_pks: bool = True, as_of: Union[pd.Timestamp, None] = None) -> pd.DataFrame:
        """GET query"""

        load_idx = load_idx or []

        table = self.table
        if self.hierarchy == 'entity':
            table = table.to_primary()

        reduced = table.inner_join_indices(load_idx, 'reduced')
        filtered = reduced.get_filtered_table(as_of)

        pks = self.primary_keys if load_pks else []
        s = filtered.select_fields(pks + fields)

        if distinct:
            s = s.distinct()

        data = pd.read_sql(s, self.engine)
        data = data.rename(columns={col.name: field for field, col in self.field_column_map.items()})
        data = data.dropna(subset=fields, how='all')

        if load_pks:
            data = data.set_index(pks)

        if 'object' in [dt.name for dt in data.dtypes]:
            _force_cast_numeric_types(data, fields)

        data = data.sort_index()

        return data

    def put(self, data: pd.DataFrame, load_cutoff: Union[pd.Timestamp, None] = None,
            load_idx_to_overwrite: List[pd.Index] = None, chunksize: int = 10000) -> int:

        if self.table._get_date_created_field() in data.columns:
            raise ValueError('date_created is a system field and cannot be used in a Put query')

        registered_non_system_fields = [field for field in self.registered_fields if not field.is_system_field]
        missing_columns = set(registered_non_system_fields).difference(set(data.columns))
        if len(missing_columns) > 0:
            raise ValueError(f'Put query must contain all app fields, missing {missing_columns}')

        if load_idx_to_overwrite is not None:
            # Add null data for existing rows
            existing_data = self.get(list(data.columns), distinct=False, load_idx=load_idx_to_overwrite)
            if len(existing_data) > 0:
                data = data.reorder_levels(existing_data.index.names)
                data = data.align(existing_data, join='outer')[0]

        df = data.reset_index()
        rename = {f: col.name for f, col in self.field_column_map.items()}
        df = df.rename(columns=rename)

        if 'load_cutoff' not in df.columns:
            df.loc[:, 'load_cutoff'] = load_cutoff

        sql_table = self.table.table

        df.to_sql(name=sql_table.name,
                  con=sql_table.bind,
                  schema=sql_table.schema,
                  if_exists='append',
                  index=False,
                  chunksize=chunksize,
                  method='multi')

        return len(df)

    def delete(self, fields: List[Field], load_idx: List[pd.Index]) -> int:
        non_system_fields = [field for field in fields if not field.is_system_field]
        registered_non_system_fields = [field for field in self.registered_fields if not field.is_system_field]
        missing_columns = set(registered_non_system_fields).difference(set(non_system_fields))
        if len(missing_columns) > 0:
            raise ValueError(f'Delete query must contain all app fields, missing {missing_columns}')

        data = self.get(fields=non_system_fields, distinct=False, load_idx=load_idx)
        deleted = pd.DataFrame(pd.np.nan, index=data.index, columns=data.columns)

        del_count = 0
        if len(deleted) > 0:
            del_count = self.put(deleted)

        return del_count

    def install(self):
        """Function to be called upon creation of a new DataInterface to persistently
        install the DataInterface and allow opening and closing it"""
        self.table.table.create(self.engine)

    def uninstall(self):
        """Function to be called upon deletion of a DataInterface"""
        self.table.table.drop()

    def open(self):
        """Function called by context manager upon open to allow for get and
        put query calls"""
        pass

    def close(self):
        """Function called by context manager upon exit to deallocate all relevant
        resources"""
        pass

    @classmethod
    def from_app(cls, app: App) -> PSQLTable:

        db_name = config['SRAYDB_SQL_DATABASE']
        schema_name = 'public'
        engine = get_engine(db_name)
        meta = get_meta(engine, schema_name)

        table_name = '__'.join([app.name, app.version_string]).lower()

        if f'{meta.schema}.{table_name}' in meta.tables:
            sql_table = meta.tables[f'{meta.schema}.{table_name}']
        else:
            sql_table = cls._create_sql_table(meta, app, table_name)

        field_column_map = {}

        for pk in app.primary_keys:
            field_column_map[pk] = sql_table.c[pk.name]
        for key in app.keys():
            field_column_map[app[key]] = sql_table.c[key]

        widetable = WideTable(sql_table, field_column_map, app.hierarchy)
        psqltable = cls(engine, field_column_map, app.hierarchy, widetable)

        return psqltable

    def get_status(self, app: App) -> Dict:
        sql_table = self.table.table
        target_table = self._create_sql_table(sql_table.metadata, app, sql_table.name)

        missing_columns = list(set(target_table.columns).difference(sql_table.columns))
        redundant_columns = list(set(sql_table.columns).difference(target_table.columns))

        status = {
            'exists': sql_table.exists(),
            'missing_columns': missing_columns,
            'redundant_columns': redundant_columns
        }

        return status

    @staticmethod
    def _create_sql_table(meta: MetaData, app: App, table_name: str) -> Table:

        pk_types = {
            # Hackity solution
            PrimaryKey.assetid: sqltypes.Integer,
            PrimaryKey.date: sqltypes.Date
        }
        columns = []

        now_utc = func.timezone('UTC', func.current_timestamp())
        date_created = Column('date_created', sqltypes.DateTime, server_default=now_utc, primary_key=True, index=False)

        columns.append(date_created)

        for i, pk in enumerate(app.primary_keys):

            col_name = pk.name.lower()
            col_type = pk_types[pk]
            col = Column(col_name, col_type, primary_key=True, nullable=False, server_default=None, index=True)
            columns.append(col)

        load_cutoff = Column('load_cutoff', sqltypes.DateTime, nullable=True, index=False)
        columns.append(load_cutoff)

        for field_name, field in app.items():
            if field.is_system_field:
                continue
            col_name = field_name.lower()
            col_type = field.type
            col = Column(col_name, col_type, primary_key=False, nullable=True, index=False)
            columns.append(col)

        if f'{meta.schema}.{table_name}' in meta.tables:
            sql_table = meta.tables[f'{meta.schema}.{table_name}']

        else:
            sql_table = Table(table_name, meta, *columns)

        return sql_table

    def _to_primary(query):
        raise NotImplementedError

    def __repr__(self):
        return f'<{self.__class__.__qualname__} on table {self.table.name}>'


def _force_cast_numeric_types(df: pd.DataFrame, fields: List[Field]):

    num_types = [Float, Integer, Numeric, BigInteger, SmallInteger]
    date_types = [Date, DateTime, Time]
    for field in fields:
        if field.type in num_types and df[field].dtype.name == 'object':
            df[field] = pd.to_numeric(df[field])
        if field.type in date_types and df[field].dtype.name == 'object':
            df[field] = pd.to_datetime(df[field])
